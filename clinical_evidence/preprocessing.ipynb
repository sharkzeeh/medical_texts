{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0e675a0-d517-4c3e-8f6f-222d7b6bcb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55adefc6-b2a9-4158-a101-acde77e357c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/uadmin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "# import pymorphy2\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "latin = re.compile(r'[^a-z ]')\n",
    "mult_ws = re.compile(r'\\s+')\n",
    "\n",
    "nltk_lemmatizer = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0010080-2355-434f-815a-da703d181e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not word_normal_form in ['.', ',', ':', '?', '!'])\n",
    "\n",
    "def lemmatize(text: str, lemmatizer, min_word_size: int = 5) -> List[str]:\n",
    "    words = nltk.word_tokenize(text)\n",
    "    res = []\n",
    "    for word in words:\n",
    "        if word not in stopwords:\n",
    "            word_normal_form = lemmatizer.lemmatize(word)\n",
    "            if len(word_normal_form) < min_word_size:# or word_normal_form.isdigit():\n",
    "                continue\n",
    "            res.append(word_normal_form)\n",
    "    return res\n",
    "\n",
    "def process_regex(re_words_to_save,\n",
    "                  re_mult_ws,\n",
    "                  text: str) -> str:\n",
    "    '''\n",
    "    строковый предпроцессинг страницы\n",
    "    '''\n",
    "    text = text.lower()\n",
    "    text = re.sub(re_words_to_save, '', text)\n",
    "    text = re.sub(re_mult_ws, ' ', text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def process_doc(doc: List[str], lemmatizer, min_word_size=3, min_sent_size=10) -> List[str]:\n",
    "    '''\n",
    "    осуществляет препроцессинг всего документа по предложениям\n",
    "    также производится лемматизация по токенам\n",
    "    '''\n",
    "    proc_doc = []\n",
    "    for sent in doc:\n",
    "        proc_sent = process_regex(\n",
    "            latin,\n",
    "            mult_ws,\n",
    "            text=sent\n",
    "        )\n",
    "        proc_sent_tokens = lemmatize(proc_sent, lemmatizer, min_word_size)\n",
    "        # минимальная длина предложения\n",
    "        if len(proc_sent_tokens) < min_sent_size:\n",
    "            continue\n",
    "        proc_sent = ' '.join(proc_sent_tokens)\n",
    "        proc_doc.append(proc_sent)\n",
    "    return proc_doc\n",
    "\n",
    "def sentenize_doc(doc: str) -> List[str]:\n",
    "    return [sent for sent in nltk.tokenize.sent_tokenize(doc) if len(sent)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53c3f08d-5061-4b74-a849-d9babf95f8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_docs(source_dir: str, min_word_size, min_sent_size) -> List:\n",
    "    docs = []\n",
    "    for doc_name in tqdm(os.listdir(source_dir)):\n",
    "        if os.path.isdir(doc_name):\n",
    "            continue\n",
    "        with open(f'{source_dir}/{doc_name}') as fh:\n",
    "            doc = fh.read()\n",
    "            doc_sentenized = sentenize_doc(doc)\n",
    "            proc_doc_sentenized = process_doc(doc_sentenized,\n",
    "                                              lemmatizer=nltk_lemmatizer,\n",
    "                                              min_word_size=min_word_size,\n",
    "                                              min_sent_size=min_sent_size)\n",
    "            docs.append(proc_doc_sentenized)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1989617-5105-413b-89f9-f44ecd0b24f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4e686a19e8d4ec59b65614bed96522f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2657 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "procced_docs = process_all_docs('txt', min_word_size=3, min_sent_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f45f3da2-a6cb-4ec4-ac5c-9e23f4d8ecc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_clean_text(doc: str, filename: str = None, target_dir: str = 'clean_txt') -> None:\n",
    "    if not os.path.exists(target_dir):\n",
    "        os.makedirs(target_dir)\n",
    "    if os.path.isdir(f'{target_dir}/{filename}'):\n",
    "        return\n",
    "    with open(f'{target_dir}/{filename}', 'w') as fh:\n",
    "        fh.writelines([f'{sent}\\n' for sent in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6919cbd6-26ee-4210-984c-26c5242122da",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc, fname in zip(procced_docs, os.listdir('txt')):\n",
    "    save_clean_text(doc, fname)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
